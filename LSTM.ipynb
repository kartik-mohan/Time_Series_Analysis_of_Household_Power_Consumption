{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Importing libraries"
      ],
      "metadata": {
        "id": "x-Jj1ldWiNCH"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4gjVczoLDFL"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn as sk\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgPilMc6vYrE"
      },
      "source": [
        "from sklearn.model_selection import train_test_split                              # to split the data into two parts\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn import metrics                                                       # for the check the error and accuracy of the model\n",
        "from sklearn.metrics import mean_squared_error,r2_score,mean_absolute_error,mean_absolute_percentage_error"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4kWlGlUwqJn"
      },
      "source": [
        "## for Deep-learing:\n",
        "import keras\n",
        "from keras.layers import Dense\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dropout"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Liw_fK5hOHXN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1241b9a6-c3da-4bfb-c2c1-0c115a75a3d7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data Preprocessing"
      ],
      "metadata": {
        "id": "NnxzofYIilol"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JDLHF7DQB52c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "8e33e61a-66f6-4ec8-f0f6-b5d5584199a5"
      },
      "source": [
        "# importing dataset\n",
        "df = pd.read_csv('/content/drive/MyDrive/SML_project/household_power_consumption.txt', sep=\";\", parse_dates={'dt' : ['Date', 'Time']}, infer_datetime_format=True, low_memory=False, na_values=['nan','?'], index_col='dt')\n",
        "df.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Global_active_power</th>\n",
              "      <th>Global_reactive_power</th>\n",
              "      <th>Voltage</th>\n",
              "      <th>Global_intensity</th>\n",
              "      <th>Sub_metering_1</th>\n",
              "      <th>Sub_metering_2</th>\n",
              "      <th>Sub_metering_3</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dt</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2006-12-16 17:24:00</th>\n",
              "      <td>4.216</td>\n",
              "      <td>0.418</td>\n",
              "      <td>234.84</td>\n",
              "      <td>18.4</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>17.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2006-12-16 17:25:00</th>\n",
              "      <td>5.360</td>\n",
              "      <td>0.436</td>\n",
              "      <td>233.63</td>\n",
              "      <td>23.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>16.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2006-12-16 17:26:00</th>\n",
              "      <td>5.374</td>\n",
              "      <td>0.498</td>\n",
              "      <td>233.29</td>\n",
              "      <td>23.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>17.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2006-12-16 17:27:00</th>\n",
              "      <td>5.388</td>\n",
              "      <td>0.502</td>\n",
              "      <td>233.74</td>\n",
              "      <td>23.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>17.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2006-12-16 17:28:00</th>\n",
              "      <td>3.666</td>\n",
              "      <td>0.528</td>\n",
              "      <td>235.68</td>\n",
              "      <td>15.8</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>17.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     Global_active_power  ...  Sub_metering_3\n",
              "dt                                        ...                \n",
              "2006-12-16 17:24:00                4.216  ...            17.0\n",
              "2006-12-16 17:25:00                5.360  ...            16.0\n",
              "2006-12-16 17:26:00                5.374  ...            17.0\n",
              "2006-12-16 17:27:00                5.388  ...            17.0\n",
              "2006-12-16 17:28:00                3.666  ...            17.0\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biOrZGQ7V0PQ",
        "outputId": "c575330c-2774-4956-a14c-7fcf9037ea95"
      },
      "source": [
        "df = df.fillna(method='ffill')\n",
        "df.isnull().sum()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Global_active_power      0\n",
              "Global_reactive_power    0\n",
              "Voltage                  0\n",
              "Global_intensity         0\n",
              "Sub_metering_1           0\n",
              "Sub_metering_2           0\n",
              "Sub_metering_3           0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['energy_consumed'] = (df['Global_active_power'] * 1000 / 60) - (df['Sub_metering_1'] + df['Sub_metering_2'] + df['Sub_metering_3'])"
      ],
      "metadata": {
        "id": "vy5TXQqjCkgZ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spaullsjaDan",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "b7e3fe41-9f73-446c-deeb-9a2a2301fecd"
      },
      "source": [
        "data_resampled = df.resample('SM').sum()\n",
        "data_resampled.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Global_active_power</th>\n",
              "      <th>Global_reactive_power</th>\n",
              "      <th>Voltage</th>\n",
              "      <th>Global_intensity</th>\n",
              "      <th>Sub_metering_1</th>\n",
              "      <th>Sub_metering_2</th>\n",
              "      <th>Sub_metering_3</th>\n",
              "      <th>energy_consumed</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dt</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2006-12-15</th>\n",
              "      <td>38332.010</td>\n",
              "      <td>2739.412</td>\n",
              "      <td>4965281.53</td>\n",
              "      <td>161961.8</td>\n",
              "      <td>27536.0</td>\n",
              "      <td>48403.0</td>\n",
              "      <td>156485.0</td>\n",
              "      <td>406442.833333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2006-12-31</th>\n",
              "      <td>34634.820</td>\n",
              "      <td>3090.402</td>\n",
              "      <td>5195859.08</td>\n",
              "      <td>146674.4</td>\n",
              "      <td>19277.0</td>\n",
              "      <td>36935.0</td>\n",
              "      <td>144067.0</td>\n",
              "      <td>376968.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2007-01-15</th>\n",
              "      <td>35512.854</td>\n",
              "      <td>2797.424</td>\n",
              "      <td>5558097.23</td>\n",
              "      <td>150307.0</td>\n",
              "      <td>37156.0</td>\n",
              "      <td>36736.0</td>\n",
              "      <td>176994.0</td>\n",
              "      <td>340994.900000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2007-01-31</th>\n",
              "      <td>34341.684</td>\n",
              "      <td>2543.752</td>\n",
              "      <td>5194277.02</td>\n",
              "      <td>144951.6</td>\n",
              "      <td>29936.0</td>\n",
              "      <td>40371.0</td>\n",
              "      <td>172991.0</td>\n",
              "      <td>329063.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2007-02-15</th>\n",
              "      <td>23996.272</td>\n",
              "      <td>2110.592</td>\n",
              "      <td>4500232.82</td>\n",
              "      <td>101306.6</td>\n",
              "      <td>17648.0</td>\n",
              "      <td>29843.0</td>\n",
              "      <td>110396.0</td>\n",
              "      <td>242050.866667</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Global_active_power  ...  energy_consumed\n",
              "dt                               ...                 \n",
              "2006-12-15            38332.010  ...    406442.833333\n",
              "2006-12-31            34634.820  ...    376968.000000\n",
              "2007-01-15            35512.854  ...    340994.900000\n",
              "2007-01-31            34341.684  ...    329063.400000\n",
              "2007-02-15            23996.272  ...    242050.866667\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f9iqoDubgH2x"
      },
      "source": [
        "Converting time series into supervised machine learning problem"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
        "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
        "\tdff = pd.DataFrame(data)\n",
        "\tcols, names = list(), list()\n",
        "\t# input sequence (t-n, ... t-1)\n",
        "\tfor i in range(n_in, 0, -1):\n",
        "\t\tcols.append(dff.shift(i))\n",
        "\t\tnames += [('var%d(t-%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "\t# forecast sequence (t, t+1, ... t+n)\n",
        "\tfor i in range(0, n_out):\n",
        "\t\tcols.append(dff.shift(-i))\n",
        "\t\tif i == 0:\n",
        "\t\t\tnames += [('var%d(t)' % (j+1)) for j in range(n_vars)]\n",
        "\t\telse:\n",
        "\t\t\tnames += [('var%d(t+%d)' % (j+1, i)) for j in range(n_vars)]\n",
        "\t# put it all together\n",
        "\tagg = pd.concat(cols, axis=1)\n",
        "\tagg.columns = names\n",
        "\t# drop rows with NaN values\n",
        "\tif dropnan:\n",
        "\t\tagg.dropna(inplace=True)\n",
        "\treturn agg"
      ],
      "metadata": {
        "id": "lYyf_yHi8UFm"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xx6894jvH1bw"
      },
      "source": [
        "#### Feature Scaling:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqoOFG_PHnEW"
      },
      "source": [
        "#Minmax standard scaling technique:\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled = scaler.fit_transform(data_resampled.values)\n",
        "\n",
        "# frame as supervised learning\n",
        "data_reframed = series_to_supervised(scaled, 1, 1)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5m9aPHUIHRH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "707f810b-6b74-4e2c-bb44-22eff122c43a"
      },
      "source": [
        "data_reframed.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>var1(t-1)</th>\n",
              "      <th>var2(t-1)</th>\n",
              "      <th>var3(t-1)</th>\n",
              "      <th>var4(t-1)</th>\n",
              "      <th>var5(t-1)</th>\n",
              "      <th>var6(t-1)</th>\n",
              "      <th>var7(t-1)</th>\n",
              "      <th>var8(t-1)</th>\n",
              "      <th>var1(t)</th>\n",
              "      <th>var2(t)</th>\n",
              "      <th>var3(t)</th>\n",
              "      <th>var4(t)</th>\n",
              "      <th>var5(t)</th>\n",
              "      <th>var6(t)</th>\n",
              "      <th>var7(t)</th>\n",
              "      <th>var8(t)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.950534</td>\n",
              "      <td>0.352012</td>\n",
              "      <td>0.554706</td>\n",
              "      <td>0.963389</td>\n",
              "      <td>0.626331</td>\n",
              "      <td>0.939737</td>\n",
              "      <td>0.586722</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.847420</td>\n",
              "      <td>0.462026</td>\n",
              "      <td>0.705259</td>\n",
              "      <td>0.859667</td>\n",
              "      <td>0.438472</td>\n",
              "      <td>0.680222</td>\n",
              "      <td>0.531019</td>\n",
              "      <td>0.919942</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.847420</td>\n",
              "      <td>0.462026</td>\n",
              "      <td>0.705259</td>\n",
              "      <td>0.859667</td>\n",
              "      <td>0.438472</td>\n",
              "      <td>0.680222</td>\n",
              "      <td>0.531019</td>\n",
              "      <td>0.919942</td>\n",
              "      <td>0.871909</td>\n",
              "      <td>0.370195</td>\n",
              "      <td>0.941778</td>\n",
              "      <td>0.884313</td>\n",
              "      <td>0.845146</td>\n",
              "      <td>0.675718</td>\n",
              "      <td>0.678720</td>\n",
              "      <td>0.822234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.871909</td>\n",
              "      <td>0.370195</td>\n",
              "      <td>0.941778</td>\n",
              "      <td>0.884313</td>\n",
              "      <td>0.845146</td>\n",
              "      <td>0.675718</td>\n",
              "      <td>0.678720</td>\n",
              "      <td>0.822234</td>\n",
              "      <td>0.839245</td>\n",
              "      <td>0.290684</td>\n",
              "      <td>0.704226</td>\n",
              "      <td>0.847978</td>\n",
              "      <td>0.680921</td>\n",
              "      <td>0.757977</td>\n",
              "      <td>0.660763</td>\n",
              "      <td>0.789826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.839245</td>\n",
              "      <td>0.290684</td>\n",
              "      <td>0.704226</td>\n",
              "      <td>0.847978</td>\n",
              "      <td>0.680921</td>\n",
              "      <td>0.757977</td>\n",
              "      <td>0.660763</td>\n",
              "      <td>0.789826</td>\n",
              "      <td>0.550714</td>\n",
              "      <td>0.154914</td>\n",
              "      <td>0.251057</td>\n",
              "      <td>0.551854</td>\n",
              "      <td>0.401419</td>\n",
              "      <td>0.519733</td>\n",
              "      <td>0.379980</td>\n",
              "      <td>0.553488</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.550714</td>\n",
              "      <td>0.154914</td>\n",
              "      <td>0.251057</td>\n",
              "      <td>0.551854</td>\n",
              "      <td>0.401419</td>\n",
              "      <td>0.519733</td>\n",
              "      <td>0.379980</td>\n",
              "      <td>0.553488</td>\n",
              "      <td>0.567277</td>\n",
              "      <td>0.228134</td>\n",
              "      <td>0.707229</td>\n",
              "      <td>0.570262</td>\n",
              "      <td>0.607997</td>\n",
              "      <td>0.940032</td>\n",
              "      <td>0.388135</td>\n",
              "      <td>0.500320</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   var1(t-1)  var2(t-1)  var3(t-1)  ...   var6(t)   var7(t)   var8(t)\n",
              "1   0.950534   0.352012   0.554706  ...  0.680222  0.531019  0.919942\n",
              "2   0.847420   0.462026   0.705259  ...  0.675718  0.678720  0.822234\n",
              "3   0.871909   0.370195   0.941778  ...  0.757977  0.660763  0.789826\n",
              "4   0.839245   0.290684   0.704226  ...  0.519733  0.379980  0.553488\n",
              "5   0.550714   0.154914   0.251057  ...  0.940032  0.388135  0.500320\n",
              "\n",
              "[5 rows x 16 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e8tfBfYnIIl4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "e0cd954e-f0f8-42dc-b12a-f34438d06afc"
      },
      "source": [
        "# drop columns we don't want to predict\n",
        "data_reframed.drop(data_reframed.columns[[9,10,11,12,13,14,15]], axis=1, inplace=True)\n",
        "print(data_reframed.head())"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-ded84773e8e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# drop columns we don't want to predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdata_reframed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_reframed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m13\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m14\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_reframed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4112\u001b[0m             \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4114\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_scalar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4116\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index 9 is out of bounds for axis 0 with size 9"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sap0uEHDfC3U"
      },
      "source": [
        "### Splitting dataset to train and test:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_reframed.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YxHQDBhL9nRW",
        "outputId": "a370d807-1591-4e81-b84a-6e8e4882d176"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(94, 9)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7isnLOlr8ofU"
      },
      "source": [
        "def split():\n",
        "  values = data_reframed.values\n",
        "\n",
        "  num_trains = 94-18\n",
        "  #train data\n",
        "  train = values[:num_trains, :]\n",
        "  #test data\n",
        "  test = values[num_trains:, :]\n",
        "\n",
        "  # split into train and test\n",
        "  train_X, train_y = train[:, :-1], train[:, -1]\n",
        "  test_X, test_y = test[:, :-1], test[:, -1]\n",
        "\n",
        "  # reshape input to be 3D [samples, timesteps, features]\n",
        "  train_X = train_X.reshape((train_X.shape[0], 1, train_X.shape[1]))\n",
        "  test_X = test_X.reshape((test_X.shape[0], 1, test_X.shape[1]))\n",
        "\n",
        "  return train_X, train_y, test_X, test_y"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = {}\n",
        "for unit in [40,70,100] :  \n",
        "  train_X, train_y, test_X, test_y = split()\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(unit, input_shape=(train_X.shape[1], train_X.shape[2])))\n",
        "  model.add(Dropout(0.2))\n",
        "  model.add(Dense(1))\n",
        "  model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "  # fit network\n",
        "  history = model.fit(train_X, train_y, epochs=80, batch_size=20, validation_data=(test_X, test_y), verbose=2, shuffle=False)\n",
        "\n",
        "  # make a prediction\n",
        "  yhat = model.predict(test_X)\n",
        "  test_X = test_X.reshape((test_X.shape[0], 8))\n",
        "\n",
        "  # invert scaling for forecast\n",
        "  inv_yhat = np.concatenate((yhat, test_X[:, -7:]), axis=1)\n",
        "  inv_yhat = scaler.inverse_transform(inv_yhat)\n",
        "  inv_yhat = inv_yhat[:,0]\n",
        "\n",
        "  # invert scaling for actual\n",
        "  test_y = test_y.reshape((len(test_y), 1))\n",
        "  inv_y = np.concatenate((test_y, test_X[:, -7:]), axis=1)\n",
        "  inv_y = scaler.inverse_transform(inv_y)\n",
        "  inv_y = inv_y[:,0]\n",
        "  \n",
        "  # calculate RMSE\n",
        "  rmse = np.sqrt(mean_squared_error(inv_y, inv_yhat))\n",
        "  mse = mean_squared_error(inv_y, inv_yhat)\n",
        "  mae = mean_absolute_error(inv_y, inv_yhat)\n",
        "  r2 = r2_score(inv_y, inv_yhat)\n",
        "  mape = mean_absolute_percentage_error(inv_y, inv_yhat)\n",
        "\n",
        "  models[unit] = [rmse, mse, mae, r2, mape]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kF3Ez-W7ANLJ",
        "outputId": "cc2042d8-267f-40c5-c659-7a0264f9143d"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/80\n",
            "4/4 - 3s - loss: 0.2864 - val_loss: 0.1739 - 3s/epoch - 838ms/step\n",
            "Epoch 2/80\n",
            "4/4 - 0s - loss: 0.2245 - val_loss: 0.1342 - 35ms/epoch - 9ms/step\n",
            "Epoch 3/80\n",
            "4/4 - 0s - loss: 0.1813 - val_loss: 0.1008 - 32ms/epoch - 8ms/step\n",
            "Epoch 4/80\n",
            "4/4 - 0s - loss: 0.1347 - val_loss: 0.0736 - 29ms/epoch - 7ms/step\n",
            "Epoch 5/80\n",
            "4/4 - 0s - loss: 0.1049 - val_loss: 0.0524 - 36ms/epoch - 9ms/step\n",
            "Epoch 6/80\n",
            "4/4 - 0s - loss: 0.0715 - val_loss: 0.0369 - 35ms/epoch - 9ms/step\n",
            "Epoch 7/80\n",
            "4/4 - 0s - loss: 0.0507 - val_loss: 0.0266 - 35ms/epoch - 9ms/step\n",
            "Epoch 8/80\n",
            "4/4 - 0s - loss: 0.0465 - val_loss: 0.0208 - 59ms/epoch - 15ms/step\n",
            "Epoch 9/80\n",
            "4/4 - 0s - loss: 0.0290 - val_loss: 0.0185 - 38ms/epoch - 9ms/step\n",
            "Epoch 10/80\n",
            "4/4 - 0s - loss: 0.0287 - val_loss: 0.0186 - 37ms/epoch - 9ms/step\n",
            "Epoch 11/80\n",
            "4/4 - 0s - loss: 0.0249 - val_loss: 0.0198 - 35ms/epoch - 9ms/step\n",
            "Epoch 12/80\n",
            "4/4 - 0s - loss: 0.0255 - val_loss: 0.0209 - 37ms/epoch - 9ms/step\n",
            "Epoch 13/80\n",
            "4/4 - 0s - loss: 0.0246 - val_loss: 0.0214 - 31ms/epoch - 8ms/step\n",
            "Epoch 14/80\n",
            "4/4 - 0s - loss: 0.0304 - val_loss: 0.0215 - 39ms/epoch - 10ms/step\n",
            "Epoch 15/80\n",
            "4/4 - 0s - loss: 0.0271 - val_loss: 0.0210 - 41ms/epoch - 10ms/step\n",
            "Epoch 16/80\n",
            "4/4 - 0s - loss: 0.0235 - val_loss: 0.0204 - 40ms/epoch - 10ms/step\n",
            "Epoch 17/80\n",
            "4/4 - 0s - loss: 0.0273 - val_loss: 0.0196 - 38ms/epoch - 9ms/step\n",
            "Epoch 18/80\n",
            "4/4 - 0s - loss: 0.0269 - val_loss: 0.0191 - 38ms/epoch - 10ms/step\n",
            "Epoch 19/80\n",
            "4/4 - 0s - loss: 0.0246 - val_loss: 0.0187 - 42ms/epoch - 10ms/step\n",
            "Epoch 20/80\n",
            "4/4 - 0s - loss: 0.0254 - val_loss: 0.0183 - 45ms/epoch - 11ms/step\n",
            "Epoch 21/80\n",
            "4/4 - 0s - loss: 0.0267 - val_loss: 0.0182 - 39ms/epoch - 10ms/step\n",
            "Epoch 22/80\n",
            "4/4 - 0s - loss: 0.0186 - val_loss: 0.0182 - 37ms/epoch - 9ms/step\n",
            "Epoch 23/80\n",
            "4/4 - 0s - loss: 0.0241 - val_loss: 0.0181 - 39ms/epoch - 10ms/step\n",
            "Epoch 24/80\n",
            "4/4 - 0s - loss: 0.0239 - val_loss: 0.0181 - 39ms/epoch - 10ms/step\n",
            "Epoch 25/80\n",
            "4/4 - 0s - loss: 0.0232 - val_loss: 0.0179 - 35ms/epoch - 9ms/step\n",
            "Epoch 26/80\n",
            "4/4 - 0s - loss: 0.0252 - val_loss: 0.0178 - 39ms/epoch - 10ms/step\n",
            "Epoch 27/80\n",
            "4/4 - 0s - loss: 0.0234 - val_loss: 0.0177 - 39ms/epoch - 10ms/step\n",
            "Epoch 28/80\n",
            "4/4 - 0s - loss: 0.0225 - val_loss: 0.0176 - 39ms/epoch - 10ms/step\n",
            "Epoch 29/80\n",
            "4/4 - 0s - loss: 0.0214 - val_loss: 0.0175 - 39ms/epoch - 10ms/step\n",
            "Epoch 30/80\n",
            "4/4 - 0s - loss: 0.0205 - val_loss: 0.0175 - 36ms/epoch - 9ms/step\n",
            "Epoch 31/80\n",
            "4/4 - 0s - loss: 0.0204 - val_loss: 0.0175 - 36ms/epoch - 9ms/step\n",
            "Epoch 32/80\n",
            "4/4 - 0s - loss: 0.0251 - val_loss: 0.0175 - 36ms/epoch - 9ms/step\n",
            "Epoch 33/80\n",
            "4/4 - 0s - loss: 0.0246 - val_loss: 0.0175 - 33ms/epoch - 8ms/step\n",
            "Epoch 34/80\n",
            "4/4 - 0s - loss: 0.0216 - val_loss: 0.0175 - 46ms/epoch - 11ms/step\n",
            "Epoch 35/80\n",
            "4/4 - 0s - loss: 0.0222 - val_loss: 0.0175 - 34ms/epoch - 9ms/step\n",
            "Epoch 36/80\n",
            "4/4 - 0s - loss: 0.0229 - val_loss: 0.0174 - 35ms/epoch - 9ms/step\n",
            "Epoch 37/80\n",
            "4/4 - 0s - loss: 0.0241 - val_loss: 0.0173 - 38ms/epoch - 9ms/step\n",
            "Epoch 38/80\n",
            "4/4 - 0s - loss: 0.0230 - val_loss: 0.0172 - 35ms/epoch - 9ms/step\n",
            "Epoch 39/80\n",
            "4/4 - 0s - loss: 0.0254 - val_loss: 0.0172 - 36ms/epoch - 9ms/step\n",
            "Epoch 40/80\n",
            "4/4 - 0s - loss: 0.0236 - val_loss: 0.0172 - 37ms/epoch - 9ms/step\n",
            "Epoch 41/80\n",
            "4/4 - 0s - loss: 0.0217 - val_loss: 0.0172 - 32ms/epoch - 8ms/step\n",
            "Epoch 42/80\n",
            "4/4 - 0s - loss: 0.0221 - val_loss: 0.0172 - 36ms/epoch - 9ms/step\n",
            "Epoch 43/80\n",
            "4/4 - 0s - loss: 0.0193 - val_loss: 0.0171 - 37ms/epoch - 9ms/step\n",
            "Epoch 44/80\n",
            "4/4 - 0s - loss: 0.0199 - val_loss: 0.0169 - 37ms/epoch - 9ms/step\n",
            "Epoch 45/80\n",
            "4/4 - 0s - loss: 0.0222 - val_loss: 0.0168 - 34ms/epoch - 8ms/step\n",
            "Epoch 46/80\n",
            "4/4 - 0s - loss: 0.0198 - val_loss: 0.0167 - 51ms/epoch - 13ms/step\n",
            "Epoch 47/80\n",
            "4/4 - 0s - loss: 0.0230 - val_loss: 0.0167 - 38ms/epoch - 10ms/step\n",
            "Epoch 48/80\n",
            "4/4 - 0s - loss: 0.0203 - val_loss: 0.0167 - 38ms/epoch - 9ms/step\n",
            "Epoch 49/80\n",
            "4/4 - 0s - loss: 0.0212 - val_loss: 0.0166 - 40ms/epoch - 10ms/step\n",
            "Epoch 50/80\n",
            "4/4 - 0s - loss: 0.0224 - val_loss: 0.0166 - 35ms/epoch - 9ms/step\n",
            "Epoch 51/80\n",
            "4/4 - 0s - loss: 0.0204 - val_loss: 0.0164 - 37ms/epoch - 9ms/step\n",
            "Epoch 52/80\n",
            "4/4 - 0s - loss: 0.0181 - val_loss: 0.0163 - 46ms/epoch - 12ms/step\n",
            "Epoch 53/80\n",
            "4/4 - 0s - loss: 0.0196 - val_loss: 0.0163 - 41ms/epoch - 10ms/step\n",
            "Epoch 54/80\n",
            "4/4 - 0s - loss: 0.0220 - val_loss: 0.0162 - 39ms/epoch - 10ms/step\n",
            "Epoch 55/80\n",
            "4/4 - 0s - loss: 0.0191 - val_loss: 0.0162 - 39ms/epoch - 10ms/step\n",
            "Epoch 56/80\n",
            "4/4 - 0s - loss: 0.0218 - val_loss: 0.0162 - 50ms/epoch - 13ms/step\n",
            "Epoch 57/80\n",
            "4/4 - 0s - loss: 0.0209 - val_loss: 0.0162 - 43ms/epoch - 11ms/step\n",
            "Epoch 58/80\n",
            "4/4 - 0s - loss: 0.0168 - val_loss: 0.0162 - 39ms/epoch - 10ms/step\n",
            "Epoch 59/80\n",
            "4/4 - 0s - loss: 0.0205 - val_loss: 0.0162 - 37ms/epoch - 9ms/step\n",
            "Epoch 60/80\n",
            "4/4 - 0s - loss: 0.0205 - val_loss: 0.0163 - 36ms/epoch - 9ms/step\n",
            "Epoch 61/80\n",
            "4/4 - 0s - loss: 0.0202 - val_loss: 0.0163 - 35ms/epoch - 9ms/step\n",
            "Epoch 62/80\n",
            "4/4 - 0s - loss: 0.0206 - val_loss: 0.0162 - 39ms/epoch - 10ms/step\n",
            "Epoch 63/80\n",
            "4/4 - 0s - loss: 0.0174 - val_loss: 0.0161 - 40ms/epoch - 10ms/step\n",
            "Epoch 64/80\n",
            "4/4 - 0s - loss: 0.0200 - val_loss: 0.0160 - 40ms/epoch - 10ms/step\n",
            "Epoch 65/80\n",
            "4/4 - 0s - loss: 0.0203 - val_loss: 0.0159 - 39ms/epoch - 10ms/step\n",
            "Epoch 66/80\n",
            "4/4 - 0s - loss: 0.0187 - val_loss: 0.0159 - 38ms/epoch - 9ms/step\n",
            "Epoch 67/80\n",
            "4/4 - 0s - loss: 0.0183 - val_loss: 0.0158 - 38ms/epoch - 10ms/step\n",
            "Epoch 68/80\n",
            "4/4 - 0s - loss: 0.0189 - val_loss: 0.0158 - 39ms/epoch - 10ms/step\n",
            "Epoch 69/80\n",
            "4/4 - 0s - loss: 0.0217 - val_loss: 0.0158 - 53ms/epoch - 13ms/step\n",
            "Epoch 70/80\n",
            "4/4 - 0s - loss: 0.0205 - val_loss: 0.0158 - 37ms/epoch - 9ms/step\n",
            "Epoch 71/80\n",
            "4/4 - 0s - loss: 0.0208 - val_loss: 0.0157 - 38ms/epoch - 9ms/step\n",
            "Epoch 72/80\n",
            "4/4 - 0s - loss: 0.0220 - val_loss: 0.0157 - 42ms/epoch - 11ms/step\n",
            "Epoch 73/80\n",
            "4/4 - 0s - loss: 0.0180 - val_loss: 0.0156 - 43ms/epoch - 11ms/step\n",
            "Epoch 74/80\n",
            "4/4 - 0s - loss: 0.0189 - val_loss: 0.0156 - 34ms/epoch - 9ms/step\n",
            "Epoch 75/80\n",
            "4/4 - 0s - loss: 0.0180 - val_loss: 0.0156 - 43ms/epoch - 11ms/step\n",
            "Epoch 76/80\n",
            "4/4 - 0s - loss: 0.0198 - val_loss: 0.0156 - 35ms/epoch - 9ms/step\n",
            "Epoch 77/80\n",
            "4/4 - 0s - loss: 0.0185 - val_loss: 0.0155 - 36ms/epoch - 9ms/step\n",
            "Epoch 78/80\n",
            "4/4 - 0s - loss: 0.0201 - val_loss: 0.0154 - 39ms/epoch - 10ms/step\n",
            "Epoch 79/80\n",
            "4/4 - 0s - loss: 0.0186 - val_loss: 0.0153 - 45ms/epoch - 11ms/step\n",
            "Epoch 80/80\n",
            "4/4 - 0s - loss: 0.0177 - val_loss: 0.0153 - 41ms/epoch - 10ms/step\n",
            "Epoch 1/80\n",
            "4/4 - 3s - loss: 0.2872 - val_loss: 0.1580 - 3s/epoch - 796ms/step\n",
            "Epoch 2/80\n",
            "4/4 - 0s - loss: 0.2203 - val_loss: 0.1113 - 35ms/epoch - 9ms/step\n",
            "Epoch 3/80\n",
            "4/4 - 0s - loss: 0.1538 - val_loss: 0.0744 - 36ms/epoch - 9ms/step\n",
            "Epoch 4/80\n",
            "4/4 - 0s - loss: 0.1091 - val_loss: 0.0471 - 38ms/epoch - 9ms/step\n",
            "Epoch 5/80\n",
            "4/4 - 0s - loss: 0.0673 - val_loss: 0.0292 - 33ms/epoch - 8ms/step\n",
            "Epoch 6/80\n",
            "4/4 - 0s - loss: 0.0458 - val_loss: 0.0199 - 33ms/epoch - 8ms/step\n",
            "Epoch 7/80\n",
            "4/4 - 0s - loss: 0.0318 - val_loss: 0.0173 - 39ms/epoch - 10ms/step\n",
            "Epoch 8/80\n",
            "4/4 - 0s - loss: 0.0247 - val_loss: 0.0191 - 41ms/epoch - 10ms/step\n",
            "Epoch 9/80\n",
            "4/4 - 0s - loss: 0.0227 - val_loss: 0.0223 - 39ms/epoch - 10ms/step\n",
            "Epoch 10/80\n",
            "4/4 - 0s - loss: 0.0251 - val_loss: 0.0245 - 39ms/epoch - 10ms/step\n",
            "Epoch 11/80\n",
            "4/4 - 0s - loss: 0.0243 - val_loss: 0.0247 - 37ms/epoch - 9ms/step\n",
            "Epoch 12/80\n",
            "4/4 - 0s - loss: 0.0318 - val_loss: 0.0233 - 39ms/epoch - 10ms/step\n",
            "Epoch 13/80\n",
            "4/4 - 0s - loss: 0.0290 - val_loss: 0.0214 - 40ms/epoch - 10ms/step\n",
            "Epoch 14/80\n",
            "4/4 - 0s - loss: 0.0217 - val_loss: 0.0198 - 38ms/epoch - 10ms/step\n",
            "Epoch 15/80\n",
            "4/4 - 0s - loss: 0.0218 - val_loss: 0.0187 - 61ms/epoch - 15ms/step\n",
            "Epoch 16/80\n",
            "4/4 - 0s - loss: 0.0217 - val_loss: 0.0180 - 35ms/epoch - 9ms/step\n",
            "Epoch 17/80\n",
            "4/4 - 0s - loss: 0.0251 - val_loss: 0.0175 - 41ms/epoch - 10ms/step\n",
            "Epoch 18/80\n",
            "4/4 - 0s - loss: 0.0246 - val_loss: 0.0174 - 38ms/epoch - 10ms/step\n",
            "Epoch 19/80\n",
            "4/4 - 0s - loss: 0.0227 - val_loss: 0.0174 - 39ms/epoch - 10ms/step\n",
            "Epoch 20/80\n",
            "4/4 - 0s - loss: 0.0215 - val_loss: 0.0174 - 37ms/epoch - 9ms/step\n",
            "Epoch 21/80\n",
            "4/4 - 0s - loss: 0.0212 - val_loss: 0.0174 - 40ms/epoch - 10ms/step\n",
            "Epoch 22/80\n",
            "4/4 - 0s - loss: 0.0215 - val_loss: 0.0175 - 41ms/epoch - 10ms/step\n",
            "Epoch 23/80\n",
            "4/4 - 0s - loss: 0.0227 - val_loss: 0.0174 - 35ms/epoch - 9ms/step\n",
            "Epoch 24/80\n",
            "4/4 - 0s - loss: 0.0227 - val_loss: 0.0173 - 36ms/epoch - 9ms/step\n",
            "Epoch 25/80\n",
            "4/4 - 0s - loss: 0.0226 - val_loss: 0.0171 - 46ms/epoch - 12ms/step\n",
            "Epoch 26/80\n",
            "4/4 - 0s - loss: 0.0218 - val_loss: 0.0170 - 35ms/epoch - 9ms/step\n",
            "Epoch 27/80\n",
            "4/4 - 0s - loss: 0.0221 - val_loss: 0.0168 - 44ms/epoch - 11ms/step\n",
            "Epoch 28/80\n",
            "4/4 - 0s - loss: 0.0200 - val_loss: 0.0166 - 44ms/epoch - 11ms/step\n",
            "Epoch 29/80\n",
            "4/4 - 0s - loss: 0.0203 - val_loss: 0.0165 - 50ms/epoch - 13ms/step\n",
            "Epoch 30/80\n",
            "4/4 - 0s - loss: 0.0206 - val_loss: 0.0164 - 38ms/epoch - 9ms/step\n",
            "Epoch 31/80\n",
            "4/4 - 0s - loss: 0.0225 - val_loss: 0.0164 - 42ms/epoch - 10ms/step\n",
            "Epoch 32/80\n",
            "4/4 - 0s - loss: 0.0217 - val_loss: 0.0163 - 39ms/epoch - 10ms/step\n",
            "Epoch 33/80\n",
            "4/4 - 0s - loss: 0.0195 - val_loss: 0.0163 - 47ms/epoch - 12ms/step\n",
            "Epoch 34/80\n",
            "4/4 - 0s - loss: 0.0208 - val_loss: 0.0163 - 42ms/epoch - 11ms/step\n",
            "Epoch 35/80\n",
            "4/4 - 0s - loss: 0.0211 - val_loss: 0.0163 - 41ms/epoch - 10ms/step\n",
            "Epoch 36/80\n",
            "4/4 - 0s - loss: 0.0213 - val_loss: 0.0162 - 42ms/epoch - 10ms/step\n",
            "Epoch 37/80\n",
            "4/4 - 0s - loss: 0.0212 - val_loss: 0.0162 - 63ms/epoch - 16ms/step\n",
            "Epoch 38/80\n",
            "4/4 - 0s - loss: 0.0199 - val_loss: 0.0162 - 42ms/epoch - 11ms/step\n",
            "Epoch 39/80\n",
            "4/4 - 0s - loss: 0.0210 - val_loss: 0.0162 - 45ms/epoch - 11ms/step\n",
            "Epoch 40/80\n",
            "4/4 - 0s - loss: 0.0210 - val_loss: 0.0161 - 46ms/epoch - 11ms/step\n",
            "Epoch 41/80\n",
            "4/4 - 0s - loss: 0.0214 - val_loss: 0.0158 - 38ms/epoch - 9ms/step\n",
            "Epoch 42/80\n",
            "4/4 - 0s - loss: 0.0186 - val_loss: 0.0157 - 45ms/epoch - 11ms/step\n",
            "Epoch 43/80\n",
            "4/4 - 0s - loss: 0.0205 - val_loss: 0.0156 - 43ms/epoch - 11ms/step\n",
            "Epoch 44/80\n",
            "4/4 - 0s - loss: 0.0192 - val_loss: 0.0156 - 40ms/epoch - 10ms/step\n",
            "Epoch 45/80\n",
            "4/4 - 0s - loss: 0.0204 - val_loss: 0.0156 - 44ms/epoch - 11ms/step\n",
            "Epoch 46/80\n",
            "4/4 - 0s - loss: 0.0199 - val_loss: 0.0156 - 43ms/epoch - 11ms/step\n",
            "Epoch 47/80\n",
            "4/4 - 0s - loss: 0.0184 - val_loss: 0.0155 - 48ms/epoch - 12ms/step\n",
            "Epoch 48/80\n",
            "4/4 - 0s - loss: 0.0206 - val_loss: 0.0155 - 49ms/epoch - 12ms/step\n",
            "Epoch 49/80\n",
            "4/4 - 0s - loss: 0.0185 - val_loss: 0.0154 - 47ms/epoch - 12ms/step\n",
            "Epoch 50/80\n",
            "4/4 - 0s - loss: 0.0193 - val_loss: 0.0155 - 47ms/epoch - 12ms/step\n",
            "Epoch 51/80\n",
            "4/4 - 0s - loss: 0.0201 - val_loss: 0.0154 - 46ms/epoch - 12ms/step\n",
            "Epoch 52/80\n",
            "4/4 - 0s - loss: 0.0176 - val_loss: 0.0153 - 46ms/epoch - 12ms/step\n",
            "Epoch 53/80\n",
            "4/4 - 0s - loss: 0.0208 - val_loss: 0.0153 - 34ms/epoch - 8ms/step\n",
            "Epoch 54/80\n",
            "4/4 - 0s - loss: 0.0200 - val_loss: 0.0152 - 39ms/epoch - 10ms/step\n",
            "Epoch 55/80\n",
            "4/4 - 0s - loss: 0.0203 - val_loss: 0.0151 - 45ms/epoch - 11ms/step\n",
            "Epoch 56/80\n",
            "4/4 - 0s - loss: 0.0191 - val_loss: 0.0151 - 41ms/epoch - 10ms/step\n",
            "Epoch 57/80\n",
            "4/4 - 0s - loss: 0.0182 - val_loss: 0.0150 - 46ms/epoch - 11ms/step\n",
            "Epoch 58/80\n",
            "4/4 - 0s - loss: 0.0180 - val_loss: 0.0150 - 49ms/epoch - 12ms/step\n",
            "Epoch 59/80\n",
            "4/4 - 0s - loss: 0.0167 - val_loss: 0.0151 - 44ms/epoch - 11ms/step\n",
            "Epoch 60/80\n",
            "4/4 - 0s - loss: 0.0177 - val_loss: 0.0150 - 44ms/epoch - 11ms/step\n",
            "Epoch 61/80\n",
            "4/4 - 0s - loss: 0.0166 - val_loss: 0.0149 - 43ms/epoch - 11ms/step\n",
            "Epoch 62/80\n",
            "4/4 - 0s - loss: 0.0178 - val_loss: 0.0148 - 35ms/epoch - 9ms/step\n",
            "Epoch 63/80\n",
            "4/4 - 0s - loss: 0.0180 - val_loss: 0.0147 - 41ms/epoch - 10ms/step\n",
            "Epoch 64/80\n",
            "4/4 - 0s - loss: 0.0168 - val_loss: 0.0146 - 46ms/epoch - 11ms/step\n",
            "Epoch 65/80\n",
            "4/4 - 0s - loss: 0.0159 - val_loss: 0.0147 - 42ms/epoch - 10ms/step\n",
            "Epoch 66/80\n",
            "4/4 - 0s - loss: 0.0184 - val_loss: 0.0147 - 44ms/epoch - 11ms/step\n",
            "Epoch 67/80\n",
            "4/4 - 0s - loss: 0.0164 - val_loss: 0.0147 - 37ms/epoch - 9ms/step\n",
            "Epoch 68/80\n",
            "4/4 - 0s - loss: 0.0171 - val_loss: 0.0146 - 40ms/epoch - 10ms/step\n",
            "Epoch 69/80\n",
            "4/4 - 0s - loss: 0.0165 - val_loss: 0.0146 - 38ms/epoch - 9ms/step\n",
            "Epoch 70/80\n",
            "4/4 - 0s - loss: 0.0176 - val_loss: 0.0145 - 45ms/epoch - 11ms/step\n",
            "Epoch 71/80\n",
            "4/4 - 0s - loss: 0.0185 - val_loss: 0.0144 - 42ms/epoch - 10ms/step\n",
            "Epoch 72/80\n",
            "4/4 - 0s - loss: 0.0168 - val_loss: 0.0144 - 37ms/epoch - 9ms/step\n",
            "Epoch 73/80\n",
            "4/4 - 0s - loss: 0.0187 - val_loss: 0.0144 - 40ms/epoch - 10ms/step\n",
            "Epoch 74/80\n",
            "4/4 - 0s - loss: 0.0183 - val_loss: 0.0145 - 40ms/epoch - 10ms/step\n",
            "Epoch 75/80\n",
            "4/4 - 0s - loss: 0.0193 - val_loss: 0.0145 - 36ms/epoch - 9ms/step\n",
            "Epoch 76/80\n",
            "4/4 - 0s - loss: 0.0175 - val_loss: 0.0145 - 43ms/epoch - 11ms/step\n",
            "Epoch 77/80\n",
            "4/4 - 0s - loss: 0.0180 - val_loss: 0.0145 - 44ms/epoch - 11ms/step\n",
            "Epoch 78/80\n",
            "4/4 - 0s - loss: 0.0155 - val_loss: 0.0144 - 37ms/epoch - 9ms/step\n",
            "Epoch 79/80\n",
            "4/4 - 0s - loss: 0.0170 - val_loss: 0.0143 - 39ms/epoch - 10ms/step\n",
            "Epoch 80/80\n",
            "4/4 - 0s - loss: 0.0171 - val_loss: 0.0142 - 45ms/epoch - 11ms/step\n",
            "Epoch 1/80\n",
            "4/4 - 3s - loss: 0.3282 - val_loss: 0.1767 - 3s/epoch - 660ms/step\n",
            "Epoch 2/80\n",
            "4/4 - 0s - loss: 0.2426 - val_loss: 0.1238 - 38ms/epoch - 10ms/step\n",
            "Epoch 3/80\n",
            "4/4 - 0s - loss: 0.1753 - val_loss: 0.0817 - 38ms/epoch - 10ms/step\n",
            "Epoch 4/80\n",
            "4/4 - 0s - loss: 0.1190 - val_loss: 0.0502 - 43ms/epoch - 11ms/step\n",
            "Epoch 5/80\n",
            "4/4 - 0s - loss: 0.0748 - val_loss: 0.0292 - 39ms/epoch - 10ms/step\n",
            "Epoch 6/80\n",
            "4/4 - 0s - loss: 0.0466 - val_loss: 0.0183 - 45ms/epoch - 11ms/step\n",
            "Epoch 7/80\n",
            "4/4 - 0s - loss: 0.0269 - val_loss: 0.0160 - 46ms/epoch - 12ms/step\n",
            "Epoch 8/80\n",
            "4/4 - 0s - loss: 0.0231 - val_loss: 0.0191 - 44ms/epoch - 11ms/step\n",
            "Epoch 9/80\n",
            "4/4 - 0s - loss: 0.0246 - val_loss: 0.0236 - 45ms/epoch - 11ms/step\n",
            "Epoch 10/80\n",
            "4/4 - 0s - loss: 0.0234 - val_loss: 0.0260 - 46ms/epoch - 12ms/step\n",
            "Epoch 11/80\n",
            "4/4 - 0s - loss: 0.0255 - val_loss: 0.0254 - 44ms/epoch - 11ms/step\n",
            "Epoch 12/80\n",
            "4/4 - 0s - loss: 0.0271 - val_loss: 0.0228 - 43ms/epoch - 11ms/step\n",
            "Epoch 13/80\n",
            "4/4 - 0s - loss: 0.0211 - val_loss: 0.0201 - 59ms/epoch - 15ms/step\n",
            "Epoch 14/80\n",
            "4/4 - 0s - loss: 0.0235 - val_loss: 0.0181 - 34ms/epoch - 8ms/step\n",
            "Epoch 15/80\n",
            "4/4 - 0s - loss: 0.0225 - val_loss: 0.0168 - 37ms/epoch - 9ms/step\n",
            "Epoch 16/80\n",
            "4/4 - 0s - loss: 0.0204 - val_loss: 0.0162 - 40ms/epoch - 10ms/step\n",
            "Epoch 17/80\n",
            "4/4 - 0s - loss: 0.0215 - val_loss: 0.0161 - 44ms/epoch - 11ms/step\n",
            "Epoch 18/80\n",
            "4/4 - 0s - loss: 0.0196 - val_loss: 0.0162 - 44ms/epoch - 11ms/step\n",
            "Epoch 19/80\n",
            "4/4 - 0s - loss: 0.0232 - val_loss: 0.0163 - 40ms/epoch - 10ms/step\n",
            "Epoch 20/80\n",
            "4/4 - 0s - loss: 0.0209 - val_loss: 0.0167 - 42ms/epoch - 11ms/step\n",
            "Epoch 21/80\n",
            "4/4 - 0s - loss: 0.0194 - val_loss: 0.0170 - 41ms/epoch - 10ms/step\n",
            "Epoch 22/80\n",
            "4/4 - 0s - loss: 0.0219 - val_loss: 0.0172 - 43ms/epoch - 11ms/step\n",
            "Epoch 23/80\n",
            "4/4 - 0s - loss: 0.0188 - val_loss: 0.0173 - 41ms/epoch - 10ms/step\n",
            "Epoch 24/80\n",
            "4/4 - 0s - loss: 0.0223 - val_loss: 0.0172 - 39ms/epoch - 10ms/step\n",
            "Epoch 25/80\n",
            "4/4 - 0s - loss: 0.0191 - val_loss: 0.0168 - 46ms/epoch - 11ms/step\n",
            "Epoch 26/80\n",
            "4/4 - 0s - loss: 0.0215 - val_loss: 0.0163 - 43ms/epoch - 11ms/step\n",
            "Epoch 27/80\n",
            "4/4 - 0s - loss: 0.0195 - val_loss: 0.0160 - 35ms/epoch - 9ms/step\n",
            "Epoch 28/80\n",
            "4/4 - 0s - loss: 0.0185 - val_loss: 0.0158 - 42ms/epoch - 10ms/step\n",
            "Epoch 29/80\n",
            "4/4 - 0s - loss: 0.0199 - val_loss: 0.0157 - 40ms/epoch - 10ms/step\n",
            "Epoch 30/80\n",
            "4/4 - 0s - loss: 0.0194 - val_loss: 0.0157 - 41ms/epoch - 10ms/step\n",
            "Epoch 31/80\n",
            "4/4 - 0s - loss: 0.0183 - val_loss: 0.0158 - 50ms/epoch - 13ms/step\n",
            "Epoch 32/80\n",
            "4/4 - 0s - loss: 0.0216 - val_loss: 0.0158 - 38ms/epoch - 9ms/step\n",
            "Epoch 33/80\n",
            "4/4 - 0s - loss: 0.0186 - val_loss: 0.0158 - 43ms/epoch - 11ms/step\n",
            "Epoch 34/80\n",
            "4/4 - 0s - loss: 0.0196 - val_loss: 0.0158 - 39ms/epoch - 10ms/step\n",
            "Epoch 35/80\n",
            "4/4 - 0s - loss: 0.0195 - val_loss: 0.0157 - 39ms/epoch - 10ms/step\n",
            "Epoch 36/80\n",
            "4/4 - 0s - loss: 0.0169 - val_loss: 0.0156 - 41ms/epoch - 10ms/step\n",
            "Epoch 37/80\n",
            "4/4 - 0s - loss: 0.0181 - val_loss: 0.0156 - 39ms/epoch - 10ms/step\n",
            "Epoch 38/80\n",
            "4/4 - 0s - loss: 0.0186 - val_loss: 0.0155 - 43ms/epoch - 11ms/step\n",
            "Epoch 39/80\n",
            "4/4 - 0s - loss: 0.0178 - val_loss: 0.0152 - 48ms/epoch - 12ms/step\n",
            "Epoch 40/80\n",
            "4/4 - 0s - loss: 0.0168 - val_loss: 0.0150 - 38ms/epoch - 9ms/step\n",
            "Epoch 41/80\n",
            "4/4 - 0s - loss: 0.0172 - val_loss: 0.0149 - 40ms/epoch - 10ms/step\n",
            "Epoch 42/80\n",
            "4/4 - 0s - loss: 0.0167 - val_loss: 0.0150 - 37ms/epoch - 9ms/step\n",
            "Epoch 43/80\n",
            "4/4 - 0s - loss: 0.0171 - val_loss: 0.0152 - 41ms/epoch - 10ms/step\n",
            "Epoch 44/80\n",
            "4/4 - 0s - loss: 0.0204 - val_loss: 0.0153 - 39ms/epoch - 10ms/step\n",
            "Epoch 45/80\n",
            "4/4 - 0s - loss: 0.0178 - val_loss: 0.0154 - 42ms/epoch - 10ms/step\n",
            "Epoch 46/80\n",
            "4/4 - 0s - loss: 0.0180 - val_loss: 0.0153 - 45ms/epoch - 11ms/step\n",
            "Epoch 47/80\n",
            "4/4 - 0s - loss: 0.0174 - val_loss: 0.0152 - 44ms/epoch - 11ms/step\n",
            "Epoch 48/80\n",
            "4/4 - 0s - loss: 0.0170 - val_loss: 0.0151 - 41ms/epoch - 10ms/step\n",
            "Epoch 49/80\n",
            "4/4 - 0s - loss: 0.0186 - val_loss: 0.0149 - 43ms/epoch - 11ms/step\n",
            "Epoch 50/80\n",
            "4/4 - 0s - loss: 0.0165 - val_loss: 0.0148 - 61ms/epoch - 15ms/step\n",
            "Epoch 51/80\n",
            "4/4 - 0s - loss: 0.0177 - val_loss: 0.0147 - 35ms/epoch - 9ms/step\n",
            "Epoch 52/80\n",
            "4/4 - 0s - loss: 0.0165 - val_loss: 0.0146 - 37ms/epoch - 9ms/step\n",
            "Epoch 53/80\n",
            "4/4 - 0s - loss: 0.0179 - val_loss: 0.0145 - 40ms/epoch - 10ms/step\n",
            "Epoch 54/80\n",
            "4/4 - 0s - loss: 0.0189 - val_loss: 0.0145 - 37ms/epoch - 9ms/step\n",
            "Epoch 55/80\n",
            "4/4 - 0s - loss: 0.0182 - val_loss: 0.0145 - 39ms/epoch - 10ms/step\n",
            "Epoch 56/80\n",
            "4/4 - 0s - loss: 0.0150 - val_loss: 0.0145 - 42ms/epoch - 11ms/step\n",
            "Epoch 57/80\n",
            "4/4 - 0s - loss: 0.0174 - val_loss: 0.0146 - 47ms/epoch - 12ms/step\n",
            "Epoch 58/80\n",
            "4/4 - 0s - loss: 0.0139 - val_loss: 0.0146 - 45ms/epoch - 11ms/step\n",
            "Epoch 59/80\n",
            "4/4 - 0s - loss: 0.0161 - val_loss: 0.0145 - 44ms/epoch - 11ms/step\n",
            "Epoch 60/80\n",
            "4/4 - 0s - loss: 0.0204 - val_loss: 0.0143 - 41ms/epoch - 10ms/step\n",
            "Epoch 61/80\n",
            "4/4 - 0s - loss: 0.0174 - val_loss: 0.0141 - 41ms/epoch - 10ms/step\n",
            "Epoch 62/80\n",
            "4/4 - 0s - loss: 0.0165 - val_loss: 0.0141 - 35ms/epoch - 9ms/step\n",
            "Epoch 63/80\n",
            "4/4 - 0s - loss: 0.0185 - val_loss: 0.0141 - 39ms/epoch - 10ms/step\n",
            "Epoch 64/80\n",
            "4/4 - 0s - loss: 0.0166 - val_loss: 0.0142 - 38ms/epoch - 10ms/step\n",
            "Epoch 65/80\n",
            "4/4 - 0s - loss: 0.0179 - val_loss: 0.0143 - 41ms/epoch - 10ms/step\n",
            "Epoch 66/80\n",
            "4/4 - 0s - loss: 0.0145 - val_loss: 0.0143 - 37ms/epoch - 9ms/step\n",
            "Epoch 67/80\n",
            "4/4 - 0s - loss: 0.0167 - val_loss: 0.0143 - 41ms/epoch - 10ms/step\n",
            "Epoch 68/80\n",
            "4/4 - 0s - loss: 0.0188 - val_loss: 0.0141 - 37ms/epoch - 9ms/step\n",
            "Epoch 69/80\n",
            "4/4 - 0s - loss: 0.0163 - val_loss: 0.0140 - 38ms/epoch - 10ms/step\n",
            "Epoch 70/80\n",
            "4/4 - 0s - loss: 0.0172 - val_loss: 0.0140 - 41ms/epoch - 10ms/step\n",
            "Epoch 71/80\n",
            "4/4 - 0s - loss: 0.0178 - val_loss: 0.0139 - 59ms/epoch - 15ms/step\n",
            "Epoch 72/80\n",
            "4/4 - 0s - loss: 0.0153 - val_loss: 0.0139 - 41ms/epoch - 10ms/step\n",
            "Epoch 73/80\n",
            "4/4 - 0s - loss: 0.0159 - val_loss: 0.0138 - 44ms/epoch - 11ms/step\n",
            "Epoch 74/80\n",
            "4/4 - 0s - loss: 0.0165 - val_loss: 0.0138 - 37ms/epoch - 9ms/step\n",
            "Epoch 75/80\n",
            "4/4 - 0s - loss: 0.0158 - val_loss: 0.0138 - 41ms/epoch - 10ms/step\n",
            "Epoch 76/80\n",
            "4/4 - 0s - loss: 0.0152 - val_loss: 0.0138 - 38ms/epoch - 9ms/step\n",
            "Epoch 77/80\n",
            "4/4 - 0s - loss: 0.0150 - val_loss: 0.0139 - 46ms/epoch - 11ms/step\n",
            "Epoch 78/80\n",
            "4/4 - 0s - loss: 0.0157 - val_loss: 0.0139 - 48ms/epoch - 12ms/step\n",
            "Epoch 79/80\n",
            "4/4 - 0s - loss: 0.0168 - val_loss: 0.0138 - 42ms/epoch - 10ms/step\n",
            "Epoch 80/80\n",
            "4/4 - 0s - loss: 0.0178 - val_loss: 0.0138 - 48ms/epoch - 12ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "models_df = pd.DataFrame.from_dict(models, orient='index', columns=['RMSE', 'MSE', 'MAE', 'R_sq', 'MAPE'])\n",
        "models_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "_icP2_jJgL2C",
        "outputId": "48b3460a-58cd-45f8-f22a-010870329998"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>RMSE</th>\n",
              "      <th>MSE</th>\n",
              "      <th>MAE</th>\n",
              "      <th>R_sq</th>\n",
              "      <th>MAPE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>4116.922385</td>\n",
              "      <td>1.694905e+07</td>\n",
              "      <td>3314.483088</td>\n",
              "      <td>0.189030</td>\n",
              "      <td>0.193665</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>4227.638616</td>\n",
              "      <td>1.787293e+07</td>\n",
              "      <td>3416.701455</td>\n",
              "      <td>0.144824</td>\n",
              "      <td>0.202435</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>4170.355515</td>\n",
              "      <td>1.739187e+07</td>\n",
              "      <td>3385.989067</td>\n",
              "      <td>0.167842</td>\n",
              "      <td>0.199793</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            RMSE           MSE          MAE      R_sq      MAPE\n",
              "40   4116.922385  1.694905e+07  3314.483088  0.189030  0.193665\n",
              "70   4227.638616  1.787293e+07  3416.701455  0.144824  0.202435\n",
              "100  4170.355515  1.739187e+07  3385.989067  0.167842  0.199793"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    }
  ]
}